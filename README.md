# LLM-Project

# ğŸ—ï¸ News Headline Classification using Traditional & Transformer-based LLMs

## ğŸš€ Overview

This repository demonstrates a unified pipeline for news headline classification across two phases:

* **PhaseÂ 01**: Traditional ML & Deep Learning (bagâ€‘ofâ€‘words, logistic regression, neural nets, LSTM, XGBoost, RandomÂ Forest)
* **PhaseÂ 02**: Transformer-based LLMs (BERT, DistilBERT, RoBERTa) with **LoRA** and **Prompt Tuning**

Targets four categories: **World**, **Sports**, **Business**, **Sci/Tech**.

---

## ğŸ“Š Dataset

Using the **AGÂ News** dataset:

* 120â€¯000 training samples
* 7â€¯600 test samples
* Balanced across 4 classes

---

## âš™ï¸ PhaseÂ 01 â€” Traditional ML & Deep Learning

| Model                    | TestÂ Accuracy |
| ------------------------ | ------------: |
| Logistic Regression (L2) |        91.64% |
| LSTM Classifier          |        90.12% |
| Deep Neural Network      |        74.96% |
| Random Forest            |        85.79% |
| XGBoost                  |        87.93% |

---

## ğŸ¤– PhaseÂ 02 â€” Transformer-based LLMs

| Model                | TestÂ Accuracy |
| -------------------- | ------------: |
| BERT                 |        94.72% |
| DistilBERT           |        94.75% |
| RoBERTa              |        94.82% |
| BERT + LoRA          |        94.54% |
| DistilBERT + LoRA    |        94.60% |
| RoBERTa + LoRA       |        94.95% |
| BERT + Prompt Tuning |        94.36% |

---

## ğŸ“Œ Key Features

* Single codebase for classical and transformer models
* **LoRA** for parameterâ€‘efficient fineâ€‘tuning
* **Prompt Tuning** with virtual tokens for fast adaptation
* Comprehensive visualizations (confusion matrices, loss curves, Zipfâ€™s law, confidence histograms)
* Manual headline prediction with seven models

---

## ğŸ§ª Getting Started

Clone the repo and run the Colab notebook:

```bash
git clone https://github.com/meesamamir/LLM-Project.git
```

ğŸ‘‰ [Open in Colab](https://colab.research.google.com/github/meesamamir/LLM-Project/blob/main/LLM_Project.ipynb)

---

## ğŸ“ˆ Results Summary

Transformer models outperform classical baselines. **RoBERTa +Â LoRA** achieved the highest F1 (94.95%), and **Prompt Tuning** offers quick adaptation with minimal parameters.

---

## ğŸ§  Lessons Learned

* Contextualized LLMs generalize without manual feature engineering
* LoRA cuts training cost with little performance loss
* Prompt tuning enables flexible task specification using text prompts

---

## ğŸ™‹â€â™‚ï¸ Authors

Nitin Nagarkar (https://github.com/nitinnagarkar25)

Meesam AmirÂ Syed (https://github.com/meesamamir)

---

## ğŸ“„ License

