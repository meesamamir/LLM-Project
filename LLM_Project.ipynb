{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meesamamir/LLM-Project/blob/main/LLM_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXgvxpheMeSc"
      },
      "source": [
        "# News Multi-Classification using NLP Techniques\n",
        "\n",
        "This project implements a comprehensive news classification system using NLP techniques. We will use the AG News dataset, which contains news headlines labeled as follows:\n",
        "\n",
        "0: World\n",
        "\n",
        "1: Sports\n",
        "\n",
        "2: Business\n",
        "\n",
        "3: Sci/Tech\n",
        "\n",
        "The goal is to first build a baseline classifier using logistic regression and then develop a neural network classifier for comparison.\n",
        "\n",
        "Below, we describe each step and provide detailed comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jSan62rTVoAd",
        "outputId": "75808a9d-14b5-4f43-c87f-b5e286c47f90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m804.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install gensim\n",
        "# !pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "Kol4oFRKNBDD",
        "outputId": "97c88cf0-1117-4318-fa71-7a6c526d0e33"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-46e97e12c23c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import random\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEKSJJQsNHi4"
      },
      "source": [
        "# 1 - Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-AP23v0Np60"
      },
      "source": [
        "## 1.1 - Data Preprocessing\n",
        "We load the AG News dataset using the Huggingface datasets library. Then, we preprocess the text headlines by converting to lowercase and using CountVectorizer to convert the text into bag-of-words features. We will later report the vocabulary size and the shape of our data matrices.\n",
        "\n",
        "TODO: Find ways to fix any training imbalances in data labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb_YwkJAOKzM"
      },
      "source": [
        "## 1.2 - Load the AG News dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MJfRMZzO1sc"
      },
      "outputs": [],
      "source": [
        "# The default split provides train and test\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"fancyzhx/ag_news\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJNiTgTPyN_o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the AG News dataset from Hugging Face\n",
        "dataset = load_dataset(\"fancyzhx/ag_news\")\n",
        "\n",
        "# Convert dataset to pandas DataFrame\n",
        "df_train = pd.DataFrame(dataset['train'])\n",
        "df_test = pd.DataFrame(dataset['test'])\n",
        "\n",
        "# Count labels in training and test sets\n",
        "train_counts = Counter(df_train['label'])\n",
        "test_counts = Counter(df_test['label'])\n",
        "\n",
        "# Print label distribution\n",
        "print(\"Training Set Label Counts:\", train_counts)\n",
        "print(\"Testing Set Label Counts:\", test_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22V6tbSSPUP3"
      },
      "source": [
        "## 1.3 - Extract training and test texts and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYpDJMCEPZEr"
      },
      "outputs": [],
      "source": [
        "# For convenience, extract training and test texts and labels\n",
        "\n",
        "train_texts = [example[\"text\"] for example in dataset[\"train\"]]\n",
        "train_labels = [example[\"label\"] for example in dataset[\"train\"]]\n",
        "\n",
        "test_texts = [example[\"text\"] for example in dataset[\"test\"]]\n",
        "test_labels = [example[\"label\"] for example in dataset[\"test\"]]\n",
        "\n",
        "print(f\"Number of training examples (full): {len(train_texts)}\")\n",
        "print(f\"Number of test examples: {len(test_texts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbZt67Elolrx"
      },
      "source": [
        "## 1.4 - Add slicing of training data\n",
        "\n",
        "Comment out the following code block to use full 120k rows for training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLA9aH1FpBeN"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "indices = list(range(len(train_texts)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "slice_indices = indices[:30000]  # Select 30k rows (approx. 25%)\n",
        "\n",
        "# slice_indices = indices[:120000]  # Select 120k rows\n",
        "\n",
        "train_texts = [train_texts[i] for i in slice_indices]\n",
        "train_labels = [train_labels[i] for i in slice_indices]\n",
        "\n",
        "print(f\"Using {len(train_texts)} training examples for experiments.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5J_hwoLhnYJ"
      },
      "source": [
        "## 1.5 - Display a snapshot of the data (first 5 rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3Ze5Sfghs2z"
      },
      "outputs": [],
      "source": [
        "snapshot = pd.DataFrame({\"Headline\": train_texts, \"Label\": train_labels}).head(5)\n",
        "print(\"Data Snapshot (first 5 rows):\")\n",
        "print(snapshot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjqSiL7xUemg"
      },
      "source": [
        "## 1.6 - Feature Extraction with Bag-of-Words / TF‑IDF\n",
        "We use ***CountVectorizer*** from scikit-learn to tokenize the headlines. Text data is converted into numerical features using **CountVectorizer**\n",
        "\n",
        "*   All words are converted to lowercase.\n",
        "*   We use a minimum document frequency to filter rare tokens.\n",
        "\n",
        "We then report the vocabulary size and the shapes of the training and test matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9wgpc8_UkDa"
      },
      "outputs": [],
      "source": [
        "# Create a CountVectorizer with lowercase conversion and minimum document frequency threshold\n",
        "vectorizer = CountVectorizer(lowercase=True, min_df=3)\n",
        "\n",
        "# Fit on training texts and transform both training and test texts\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_test = vectorizer.transform(test_texts)\n",
        "\n",
        "# Report the vocabulary size and matrix shapes\n",
        "vocab_size = len(vectorizer.vocabulary_)\n",
        "print(f\"\\nVocabulary size: {vocab_size}\\n\")\n",
        "print(f\"Shape of training data matrix: {X_train.shape}\") # (n_train, vocab_size)\n",
        "print(f\"Shape of test data matrix: {X_test.shape}\")      # (n_test, vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emJ-c2XUWM1k"
      },
      "source": [
        "# 2 - Baseline Classifier: Logistic Regression\n",
        "\n",
        "We implement a baseline logistic regression classifier.\n",
        "\n",
        "*   First, we train without regularization.\n",
        "*   Then, we train with L2 regularization.\n",
        "\n",
        "We report the classification accuracies on the training and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXFQXzE-WoXU"
      },
      "source": [
        "## 2.1 - Logistic Regression **without** Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXe47gerWrVB"
      },
      "outputs": [],
      "source": [
        "print(\"\\n### Logistic Regression Baseline (No Regularization) ###\\n\")\n",
        "\n",
        "# Instantiate logistic regression with a very high C to effectively turn off regularization\n",
        "lr_no_reg = LogisticRegression(fit_intercept=True, C=1e9, solver=\"liblinear\", max_iter=1000)\n",
        "\n",
        "# Train the classifier\n",
        "lr_no_reg.fit(X_train, train_labels)\n",
        "\n",
        "# Predict on training and test sets\n",
        "train_preds_no_reg = lr_no_reg.predict(X_train)\n",
        "test_preds_no_reg = lr_no_reg.predict(X_test)\n",
        "\n",
        "# Computing accuracy scores\n",
        "train_acc_no_reg = accuracy_score(train_labels, train_preds_no_reg)\n",
        "test_acc_no_reg = accuracy_score(test_labels, test_preds_no_reg)\n",
        "\n",
        "print(f\"Training Accuracy (No Reg): {train_acc_no_reg:.4f}\")\n",
        "print(f\"Test Accuracy (No Reg): {test_acc_no_reg:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejkpn6knXD8w"
      },
      "source": [
        "## 2.2 - Logistic Regression with L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca6683elXMUC"
      },
      "outputs": [],
      "source": [
        "print(\"\\n### Logistic Regression Baseline (With L2 Regularization, lambda=10.0) ###\\n\")\n",
        "\n",
        "# For L2 regularization with lambda=10.0, set C = 1/lambda = 0.1.\n",
        "lr_reg = LogisticRegression(fit_intercept=True, C=0.1, solver=\"liblinear\", max_iter=1000)\n",
        "\n",
        "# Train the classifier\n",
        "lr_reg.fit(X_train, train_labels)\n",
        "\n",
        "# Predict on training and test sets\n",
        "train_preds_reg = lr_reg.predict(X_train)\n",
        "test_preds_reg = lr_reg.predict(X_test)\n",
        "\n",
        "# Computing accuracy scores\n",
        "train_acc_reg = accuracy_score(train_labels, train_preds_reg)\n",
        "test_acc_reg = accuracy_score(test_labels, test_preds_reg)\n",
        "\n",
        "print(f\"Training Accuracy (With Reg): {train_acc_reg:.4f}\")\n",
        "print(f\"Test Accuracy (With Reg): {test_acc_reg:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TUDbEKEamzt"
      },
      "source": [
        "## 2.3 - Additional Evaluation (Precision, Recall, F1-score, Confusion Matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IGWY8TKar5m"
      },
      "outputs": [],
      "source": [
        "print(\"\\nClassification Report (Test, With Regularization):\")\n",
        "print(classification_report(test_labels, test_preds_reg))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_labels, test_preds_reg))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMYGUq8DiT6K"
      },
      "source": [
        "## 2.4 - Confusion Matrix Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL8WuNb7iaFF"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "label_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "cm = confusion_matrix(test_labels, test_preds_reg)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
        "plt.title(\"Confusion Matrix - Logistic Regression (With Reg)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yDt8KJlXXdn"
      },
      "source": [
        "# 3 - Neural Network Classifier\n",
        "\n",
        "Next, we implement a simple feedforward neural network classifier for the news classification task. The architecture is as follows:\n",
        "\n",
        "1. Embedding Layer: Converts word indices to dense vectors.\n",
        "\n",
        "2. First Hidden Layer: Sums the embeddings over the sequence and applies a linear transformation followed by a sigmoid activation.\n",
        "\n",
        "3. Second Hidden Layer: Applies another linear transformation and sigmoid activation.\n",
        "\n",
        "4. Output Layer: Applies a final linear transformation to predict the logits for the 4 classes, followed by softmax during evaluation.\n",
        "\n",
        "We use PyTorch to implement the model.\n",
        "\n",
        "Next, we will then train the network and evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgvn9s-EccpO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Setting torch seed for reproducibility\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hTfViRxcyhT"
      },
      "source": [
        "## 3.1 - Custom Dataset and DataLoader\n",
        "We define a custom dataset to process news headlines. Each headline is tokenized (using a basic split), mapped to indices using the vocabulary from CountVectorizer, and then finally padded/truncated to a fixed length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgrZopXWc1lT"
      },
      "outputs": [],
      "source": [
        "# Building a word-to-index mapping from CountVectorizer's vocabulary\n",
        "# Note: CountVectorizer vocabulary maps: word -> index.\n",
        "\n",
        "vocab = vectorizer.vocabulary_\n",
        "\n",
        "# We assume the padding index is 0. If \"<pad>\" is not in the vocabulary, we can use 0 as default.\n",
        "# (In practice, you might add a dedicated pad token to the vocabulary.)\n",
        "pad_idx = 0\n",
        "default_idx = 0 # Unknown words get index 0\n",
        "\n",
        "# Define maximum sequence length (e.g., 20 tokens)\n",
        "max_seq_len = 20\n",
        "\n",
        "# Convert text to lowercase and split on whitespace.\n",
        "def tokenize_text(text):\n",
        "  tokens = text.lower().split()\n",
        "  indices = [vocab.get(token, default_idx) for token in tokens]\n",
        "\n",
        "  return indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XtDLb6-eZt0"
      },
      "outputs": [],
      "source": [
        "class AGNewsDataset(Dataset):\n",
        "  def __init__(self, texts, labels, max_len=max_seq_len):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    indices = tokenize_text(self.texts[idx])\n",
        "    if len(indices) < self.max_len:\n",
        "        indices = indices + [pad_idx] * (self.max_len - len(indices))\n",
        "    else:\n",
        "        indices = indices[:self.max_len]\n",
        "    label = self.labels[idx]\n",
        "\n",
        "    return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr4egsKbeXvS"
      },
      "outputs": [],
      "source": [
        "# Create dataset and DataLoader instances\n",
        "train_dataset = AGNewsDataset(train_texts, train_labels, max_len=max_seq_len)\n",
        "test_dataset = AGNewsDataset(test_texts, test_labels, max_len=max_seq_len)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEswUh_1euq_"
      },
      "outputs": [],
      "source": [
        "class NeuralClassifier(nn.Module):\n",
        "  # Initialize the neural network.\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, num_classes=4):\n",
        "    super(NeuralClassifier, self).__init__()\n",
        "\n",
        "    \"\"\" Initialize the neural network.\n",
        "    Args:\n",
        "        vocab_size (int): Size of the vocabulary.\n",
        "        embed_size (int): Dimensionality of word embeddings.\n",
        "        hidden_size (int): Size of hidden layers.\n",
        "        num_classes (int): Number of news categories.\n",
        "    \"\"\"\n",
        "    # Defined layers as per 'Neural Networks for NLP.pdf' slides 27-29:\n",
        "        # Step 1: Embedding layer (with padding support)\n",
        "        # Step 2: Linear layer from embed_size to hidden_size (first hidden layer)\n",
        "        # Step 3: Second linear layer from hidden_size to hidden_size (second hidden layer)\n",
        "        # Step 4: Output linear layer from hidden_size to class_size\n",
        "\n",
        "    # Add more layers here:\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
        "    self.fc1 = nn.Linear(embed_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fc_out = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass of the model.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input tensor of shape [batch_size, seq_len].\n",
        "\n",
        "    Returns:\n",
        "        logits (Tensor): Logits of shape [batch_size, num_classes].\n",
        "    \"\"\"\n",
        "    embedded = self.embedding(x)               # [B, L, E]\n",
        "    summed = embedded.sum(dim=1)               # [B, E] fixed-size representation\n",
        "    h1 = torch.sigmoid(self.fc1(summed))       # first hidden layer\n",
        "    h2 = torch.sigmoid(self.fc2(h1))           # second hidden layer\n",
        "    logits = self.fc_out(h2)                   # output layer (logits)\n",
        "\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVxFAe9hf28P"
      },
      "source": [
        "## 3.2 - Training and Evaluation Functions\n",
        "We define functions for training one mini-batch and evaluating the model over an entire DataLoader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNGtcd1ThOMi"
      },
      "source": [
        "### 3.2.1 - Training Function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qkbTvZtf6U4"
      },
      "outputs": [],
      "source": [
        "def train_batch(batch, model, optimizer):\n",
        "  \"\"\" Train on one mini-batch\n",
        "  Args:\n",
        "    batch (tuple): (texts, labels) from DataLoader.\n",
        "    model (nn.Module): Neural network.\n",
        "    optimizer: Optimizer instance.\n",
        "\n",
        "  Returns:\n",
        "      loss (float): Mini-batch loss.\n",
        "  \"\"\"\n",
        "  # set in training mode\n",
        "  model.train()\n",
        "\n",
        "  # initialize optimizer\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # forward: prediction\n",
        "  texts, labels = batch\n",
        "  logits = model(texts)\n",
        "  loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "  # backward: gradient computation\n",
        "  loss.backward()\n",
        "\n",
        "  # norm clipping, in case the gradient norm is too large\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "  # gradient-based update parameter\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwzlxshhhXf7"
      },
      "source": [
        "### 3.2.2 - Evaluating Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH7UWnadhHxm"
      },
      "outputs": [],
      "source": [
        "def evaluate(loader, model):\n",
        "  \"\"\" Evaluate model on given DataLoader.\n",
        "  Args:\n",
        "    loader (DataLoader): Evaluation data.\n",
        "    model (nn.Module): Neural network.\n",
        "\n",
        "  Returns:\n",
        "    avg_loss (float): Average loss.\n",
        "    accuracy (float): Classification accuracy.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "  total_correct = 0\n",
        "  total_examples = 0\n",
        "  with torch.no_grad():\n",
        "      for texts, labels in loader:\n",
        "          logits = model(texts)\n",
        "          loss = F.cross_entropy(logits, labels)\n",
        "          total_loss += loss.item() * texts.size(0)\n",
        "          preds = torch.argmax(logits, dim=1)\n",
        "          total_correct += (preds == labels).sum().item()\n",
        "          total_examples += texts.size(0)\n",
        "  avg_loss = total_loss / total_examples\n",
        "  accuracy = total_correct / total_examples\n",
        "\n",
        "  return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5hZeXgYiJgx"
      },
      "source": [
        "## 3.3 - Training the Neural Network Classifier\n",
        "Hyperparameters:\n",
        "\n",
        "- Embedding size: 64\n",
        "\n",
        "- Hidden size: 64\n",
        "\n",
        "- Optimizer: SGD with learning rate 0.05\n",
        "\n",
        "- Epochs: 50 (for demonstration; increase as needed)\n",
        "\n",
        "We record the training loss and validation performance at each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_Z-W3PEiQ3M"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "embed_size = 64\n",
        "hidden_size = 64\n",
        "num_classes = 4\n",
        "num_epochs = 50 # Increase for final training\n",
        "learning_rate = 0.05\n",
        "\n",
        "# Initialize the neural network model\n",
        "model_nn = NeuralClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    embed_size=embed_size,\n",
        "    hidden_size=hidden_size,\n",
        "    num_classes=num_classes)\n",
        "\n",
        "# Use SGD optimizer\n",
        "optimizer_nn = torch.optim.SGD(\n",
        "    model_nn.parameters(), lr=learning_rate)\n",
        "\n",
        "# Record training progress\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "print(\"\\n### Training Neural Network Classifier ###\\n\")\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # 1. Train for one epoch\n",
        "    epoch_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        loss = train_batch(batch, model_nn, optimizer_nn)\n",
        "        epoch_loss += loss\n",
        "\n",
        "    # 2. Compute average training loss for this epoch\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # 3. Evaluate on the test/validation set\n",
        "    val_loss, val_acc = evaluate(test_loader, model_nn)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # 4. Print a single summary line for this epoch\n",
        "    print(f\"Epoch {epoch}: \"\n",
        "          f\"Train Loss = {avg_train_loss:.4f}, \"\n",
        "          f\"Val Loss = {val_loss:.4f}, \"\n",
        "          f\"Val Accuracy = {val_acc:.4f}\")\n",
        "\n",
        "print(\"\\nNeural Network Training Completed.\")\n",
        "print(f\"Best Validation Accuracy: {max(val_accuracies):.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracies[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9VvQi2EUAEP"
      },
      "source": [
        "## 3.4 - Visualization and Summary\n",
        "We plot the training and validation loss curves. In addition, note that evaluation metrics such as precision, recall, and F1-score (as well as a confusion matrix) should be computed for a thorough evaluation. For language model-based approaches, perplexity would also be computed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F0bMHLxi_z_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, num_epochs+1), train_losses, color=\"red\", label=\"Training Loss\")\n",
        "plt.plot(range(1, num_epochs+1), val_losses, color=\"blue\", label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs. Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s17oFTJjdQR"
      },
      "source": [
        "# 4 - Additional Experiments and Quantitative Analysis\n",
        "This section performs further analysis on the dataset:\n",
        "1. Plotting the frequency distribution of words (to check Zipf’s law).\n",
        "2. Generating word embeddings using Word2Vec and displaying similar words.\n",
        "3. Summarizing performance metrics in a table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hokGDq2tjwfn"
      },
      "source": [
        "## 4.1 - Frequency Distribution Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za6b1IZWj1RR"
      },
      "outputs": [],
      "source": [
        "# Flatten all tokens from training texts (using simple whitespace split)\n",
        "all_tokens = []\n",
        "for text in train_texts:\n",
        "    all_tokens.extend(text.lower().split())\n",
        "\n",
        "token_counts = Counter(all_tokens)\n",
        "\n",
        "# Sort tokens by frequency\n",
        "sorted_counts = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "tokens, frequencies = zip(*sorted_counts)\n",
        "\n",
        "# Plot rank-frequency (log-log scale)\n",
        "ranks = range(1, len(frequencies) + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.loglog(ranks, frequencies, marker=\".\")\n",
        "plt.xlabel(\"Rank\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Word Frequency Distribution (Zipf's Law)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p86lD-2LSPC"
      },
      "source": [
        "Frequency INVERSLY PROPORTIONAL to Rank\n",
        "\n",
        "Hence, model follows Zipf's Law"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAC4I-rskieg"
      },
      "source": [
        "##4.2 Word Embeddings using Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFoxlWBPkqVy"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my2-hFuikmu8"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize the training texts (simple whitespace split)\n",
        "tokenized_texts = [text.lower().split() for text in train_texts]\n",
        "\n",
        "# Train Word2Vec model (using a small dimension for demonstration)\n",
        "w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=50, window=5, min_count=3, workers=4, seed=42)\n",
        "\n",
        "# Display top 5 similar words for selected keywords\n",
        "keywords = [\"economy\", \"games\", \"technology\", \"market\"]\n",
        "for word in keywords:\n",
        "    if word in w2v_model.wv:\n",
        "        similar = w2v_model.wv.most_similar(word, topn=5)\n",
        "        print(f\"\\nTop similar words to '{word}':\")\n",
        "        for sim_word, score in similar:\n",
        "            print(f\"  {sim_word}: {score:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\n'{word}' not found in vocabulary.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPgOZic9mdH3"
      },
      "source": [
        "## 4.3 Performance Comparison Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww5_VHS5miop"
      },
      "outputs": [],
      "source": [
        "# Create a summary table for performance metrics\n",
        "import pandas as pd\n",
        "\n",
        "# We have baseline (lr_reg) and neural network (model_nn) performance metrics.\n",
        "# Here we use the test accuracy from earlier experiments.\n",
        "performance_data = {\n",
        "    \"Model\": [\"Logistic Regression (With Reg)\", \"Neural Network\"],\n",
        "    \"Test Accuracy\": [test_acc_reg, val_accuracies[-1]]\n",
        "}\n",
        "\n",
        "performance_df = pd.DataFrame(performance_data)\n",
        "print(\"Performance Comparison:\")\n",
        "print(performance_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvy6iMKimqJ7"
      },
      "source": [
        "# 5 - Error Analysis\n",
        "Below, we discuss examples where the models fail. For instance, consider headlines that are misclassified by the baseline model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga8EDBSlmzjy"
      },
      "outputs": [],
      "source": [
        "# For demonstration, print 5 misclassified examples from logistic regression with regularization\n",
        "misclassified = []\n",
        "for text, true_label, pred_label in zip(test_texts, test_labels, test_preds_reg):\n",
        "    if true_label != pred_label:\n",
        "        misclassified.append((text, true_label, pred_label))\n",
        "    if len(misclassified) >= 5:\n",
        "        break\n",
        "\n",
        "print(\"Examples of Misclassified Headlines (Logistic Regression with Reg):\")\n",
        "for idx, (text, true_label, pred_label) in enumerate(misclassified, start=1):\n",
        "    print(f\"\\nExample {idx}:\")\n",
        "    print(f\"Headline: {text}\")\n",
        "    print(f\"True Label: {true_label} ({label_names[true_label]})\")\n",
        "    print(f\"Predicted Label: {pred_label} ({label_names[pred_label]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGxzjR7YKg9-"
      },
      "source": [
        "#6 - Add more models here\n",
        "\n",
        "Compare our models with models that are already in the market e.g. XGBOOST, RandomForest\n",
        "\n",
        "Find standard/best methods to report results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCeAA3dLx6DJ"
      },
      "source": [
        "#Manual Headline Prediction\n",
        "Enter a news headline manually and see the predictions from:\n",
        "1. Baseline Logistic Regression (using CountVectorizer features)\n",
        "2. Neural Network Classifier (using our custom tokenization and padding)\n",
        "\n",
        "The output includes the predicted text label and the confidence (as a percentage).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KHZwvAhx-jd"
      },
      "outputs": [],
      "source": [
        "# Define label mapping\n",
        "label_mapping = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
        "\n",
        "# Input: Manually enter a headline\n",
        "headline = \"Local sports team wins championship in a stunning upset\"\n",
        "print(f\"Headline: {headline}\\n\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# ---- Baseline Prediction using Logistic Regression ----\n",
        "# Vectorize the headline using the previously fitted CountVectorizer\n",
        "X_input = vectorizer.transform([headline])\n",
        "\n",
        "# Get probability estimates\n",
        "probs_baseline = lr_reg.predict_proba(X_input)[0]\n",
        "\n",
        "# Determine the predicted label and confidence\n",
        "pred_label_baseline = lr_reg.predict(X_input)[0]\n",
        "confidence_baseline = np.max(probs_baseline) * 100\n",
        "\n",
        "print(\"Baseline Logistic Regression:\")\n",
        "print(f\" Predicted Label: {pred_label_baseline}: {label_mapping[pred_label_baseline]}\")\n",
        "print(f\" Confidence: {confidence_baseline:.2f}%\")\n",
        "print()\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# ---- Neural Network Prediction ----\n",
        "# Tokenize the headline using our custom function\n",
        "indices = tokenize_text(headline)\n",
        "# Pad or truncate the sequence to max_seq_len\n",
        "if len(indices) < max_seq_len:\n",
        "    indices = indices + [pad_idx] * (max_seq_len - len(indices))\n",
        "else:\n",
        "    indices = indices[:max_seq_len]\n",
        "# Convert to a torch tensor and add batch dimension\n",
        "input_tensor = torch.tensor([indices], dtype=torch.long)\n",
        "\n",
        "# Predict with the neural network\n",
        "model_nn.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model_nn(input_tensor)\n",
        "    # Apply softmax to obtain probability distribution\n",
        "    probs_nn = F.softmax(logits, dim=1)[0]\n",
        "    pred_label_nn = torch.argmax(probs_nn).item()\n",
        "    confidence_nn = probs_nn[pred_label_nn].item() * 100\n",
        "\n",
        "print(\"Neural Network Classifier:\")\n",
        "print(f\" Predicted Label: {pred_label_nn}: {label_mapping[pred_label_nn]}\")\n",
        "print(f\" Confidence: {confidence_nn:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7 - LSTM "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
